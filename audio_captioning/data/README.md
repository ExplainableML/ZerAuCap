## Data preparation, descriptive statistics and keyword lists

### Catalogue

* <a href='#data_prep'>1. Data preperation</a>
  * <a href='#audiocaps'>1.1. AudioCaps</a>
  * <a href='#clotho'>1.2. Clotho</a>
* <a href='#summary_stats'>2. Descriptive statistics</a>
* <a href='#keyword_lists'>3. Keyword lists</a>
  * <a href='#AudioSet'>3.1. AudioSet KW</a>
  * <a href='#CGPT'>3.2. ChatGPT KW</a>
  * <a href='#Audioset+CGPT'>3.3. AudioSet+ChatGPT KW</a>

The original MAGIC repo uses ``.json`` files to match GT captions with their respective file name. Therefore, the file name ("sound_name") in the ``.json`` files have to be equal to the actual files, where the actual file is stored ``softlinks/AudioCaps_data``.

Here, we provide the scripts that we have used to transform the .csv files from AudioCaps and Clotho to the required .json files. This is an example of AudioSet's validation set. The file ``softlinks/AudioCaps_data/rqfQRErjfk8_170000.wav`` is associated with these GT captions:

```json
{
        "split": "val",
        "sound_name": "rqfQRErjfk8_170000.wav",
        "file_path": "WHAT_IS_THIS?",
        "captions": [
            "A large crowd cheers and applauds",
            "An audience screams and gives applause",
            "Continuous applause and cheering",
            "An audience screams and gives applause",
            "An audience cheers"
        ]
    }
```

<span id='data_prep'/>

**Disclaimer**: you can ignore the data preparation part if you use the .json files in the respective .sh inference script (--GT_captions_AudioCaps) that we have provided to match a file name with its ground truth captions.

However, if you want to generate the .json files yourself, you can use the scripts that we have used.

****
<span id='audiocaps'/>

#### 1.1 AudioCaps

```bash
    .
    ├── ./AudioCaps/                    
        ├── AudioCaps_train.json # Contains the training set text captions of AudioCaps
        ├── AudioCaps_val.json # Contains the validation set text captions of AudioCaps
        ├── AudioCaps_test.json # Contains the test set text captions of AudioCaps
        └── AudioCaps_code_exp.json # Contains a small directory for testing whether the code runs
```

##### Creating the .json files

1. Download the .csv files from the AudioCaps repo: <https://github.com/cdjkim/audiocaps/tree/master/dataset>
2. Put them into

```
audio_captioning/data/raw_data/caption_datasets/AudioCaps
```

3. Run ```process_AudioCaps.py```

****

<span id='clotho'/>

#### 1.2 Clotho

```bash
    .
    ├── ./Clotho/                    
        ├── clotho_v2.1_train.json # Contains the training set text captions of Clotho
        ├── clotho_v2.1_val.json # Contains the validation set text captions of Clotho
        └── clotho_v2.1_test.json # Contains the test set text captions of Clotho
```

1. Download the .csv files of Clotho v2.1 (e.g. clotho_captions_evaluation.csv) from the Clotho webpage: <https://zenodo.org/record/4783391>
2. Put them into ``audio_captioning/data/raw_data/caption_datasets/clotho_v2.1``
3. Run ```process_clotho_v2.1.py```

**Disclaimer**: We refer to Clotho's evaluation set as Clotho's test set.

****
<span id='summary_stats'/>

### 2. Descriptive statistics

Table 3 in the paper contains summary statistics of all datasets that we have used to produce the results. We have used the script ``audio_captioning/data/summary_stats_data.py`` to create the pre-print of the LaTeX table. If you want to replicate this, make sure the data is in the right folder, according to the main README.md.

****
<span id='keyword_lists'/>

### 3. Keyword lists

As outlined in the paper, we have used AudioSet keywords and ChatGPT created keywords. We now show provide notes on how obtained these lists, in addition to the dissertation.

<span id='AudioSet'/>

#### 3.1 AudioSet KW

We have downloaded this list from here:
<http://storage.googleapis.com/us_audioset/youtube_corpus/v1/csv/class_labels_indices.csv>

It is stored in ``audio_captioning/data/AudioSet/class_labels_indices.csv``.

<span id='CGPT'/>

#### 3.2 ChatGPT KW

In order to create the list, we have ran the script ``audio_captioning/sounding_object_generator.py``.

The list containing **only** the tags generated by ChatGPT, which is the result of running the aforementioned script can be accessed in ``audio_captioning/data/sounding_objects/chatgpt_audio_tags.csv``.

****

<span id='Audioset+CGPT'/>

#### 3.3 AudioSet+ChatGPT KW

We then use this snippet to separate AudioSet tags that consisted of two single tags and concatenate the original list with the ChatGPT generated list:
<https://github.com/ExplainableML/2023-audiocaptioning-msc-stefan/blob/fc0e28574d8c07d159de1ccc9774c00d55a9a040/audio_captioning/inference_magic.py#L187-L191>
